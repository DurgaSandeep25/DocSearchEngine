{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from util import *\n",
    "import time\n",
    "from evaluation import Evaluation\n",
    "import string\n",
    "#from gensim.parsing.preprocessing import STOPWORDS\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read queries\n",
    "queries_json = json.load(open( \".\\cranfield\\cran_queries.json\", 'r'))[:]\n",
    "query_ids, queries = [item[\"query number\"] for item in queries_json], \\\n",
    "                        [item[\"query\"] for item in queries_json]\n",
    "\n",
    "# Read documents\n",
    "docs_json = json.load(open(\".\\cranfield\\cran_docs.json\", 'r'))[:]\n",
    "doc_ids, docs = [item[\"id\"] for item in docs_json], \\\n",
    "                        [item[\"title\"] for item in docs_json]\n",
    "\n",
    "qrels = json.load(open( \"./cranfield/cran_qrels.json\", 'r'))[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>experimental investigation of the aerodynamics...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>simple shear flow past a flat plate in an inco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the boundary layer in simple shear flow past a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>approximate solutions of the incompressible la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>one-dimensional transient heat conduction into...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title\n",
       "0  experimental investigation of the aerodynamics...\n",
       "1  simple shear flow past a flat plate in an inco...\n",
       "2  the boundary layer in simple shear flow past a...\n",
       "3  approximate solutions of the incompressible la...\n",
       "4  one-dimensional transient heat conduction into..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df = pd.DataFrame(docs_json).drop(['author','bibliography','id','body'],axis=1)\n",
    "docs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    import re\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"you'll\", \"you will\", text)\n",
    "    text = re.sub(r\"i'll\", \"i will\", text)\n",
    "    text = re.sub(r\"she'll\", \"she will\", text)\n",
    "    text = re.sub(r\"he'll\", \"he will\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"there's\", \"there is\", text)\n",
    "    text = re.sub(r\"here's\", \"here is\", text)\n",
    "    text = re.sub(r\"who's\", \"who is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"shouldn't\", \"should not\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"[^a-z]\", \" \", text) # This removes anything other than lower case letters(very imp)\n",
    "    text = re.sub(r\"   \", \" \", text) # Remove any extra spaces\n",
    "    return text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"custom function to remove the punctuation\"\"\"\n",
    "    PUNCT_TO_REMOVE = string.punctuation + '“' + '”'+'’' + '_'\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df = pd.DataFrame(queries,columns = ['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df['preprocessed'] = docs_df['title'].str.lower()\n",
    "query_df['preprocessed'] = query_df['query'].str.lower()\n",
    "\n",
    "docs_df['preprocessed'] = docs_df['preprocessed'].apply(clean_text)\n",
    "query_df['preprocessed'] = query_df['preprocessed'].apply(clean_text)\n",
    "\n",
    "docs_df['preprocessed'] = docs_df['preprocessed'].apply(lambda text: remove_punctuation(text))\n",
    "query_df['preprocessed'] = query_df['preprocessed'].apply(lambda text: remove_punctuation(text))\n",
    "\n",
    "docs_df['preprocessed'] = docs_df['preprocessed'].apply(lambda text: remove_stopwords(text))\n",
    "query_df['preprocessed'] = query_df['preprocessed'].apply(lambda text: remove_stopwords(text))\n",
    "\n",
    "docs_df['preprocessed'] = docs_df['preprocessed'].apply(lambda text: lemmatize_words(text))\n",
    "query_df['preprocessed'] = query_df['preprocessed'].apply(lambda text: lemmatize_words(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>experimental investigation of the aerodynamics...</td>\n",
       "      <td>experimental investigation aerodynamics wing s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>simple shear flow past a flat plate in an inco...</td>\n",
       "      <td>simple shear flow past flat plate incompressib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the boundary layer in simple shear flow past a...</td>\n",
       "      <td>boundary layer simple shear flow past flat plate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>approximate solutions of the incompressible la...</td>\n",
       "      <td>approximate solution incompressible laminar bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>one-dimensional transient heat conduction into...</td>\n",
       "      <td>one dimensional transient heat conduction doub...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  experimental investigation of the aerodynamics...   \n",
       "1  simple shear flow past a flat plate in an inco...   \n",
       "2  the boundary layer in simple shear flow past a...   \n",
       "3  approximate solutions of the incompressible la...   \n",
       "4  one-dimensional transient heat conduction into...   \n",
       "\n",
       "                                        preprocessed  \n",
       "0  experimental investigation aerodynamics wing s...  \n",
       "1  simple shear flow past flat plate incompressib...  \n",
       "2   boundary layer simple shear flow past flat plate  \n",
       "3  approximate solution incompressible laminar bo...  \n",
       "4  one dimensional transient heat conduction doub...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = docs_df['preprocessed'].tolist() + query_df['preprocessed'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluation()\n",
    "def Evaluation_metrics(doc_IDs_ordered, query_ids, qrels, n_comp, op_folder = './',save_results = 2, verbose = 1):\n",
    "    \"\"\"\n",
    "    save_results : 0    ===> don't save anything\n",
    "                 : 1    ===> just save results\n",
    "                 : > 2  ===> save plots also\n",
    "    \"\"\"\n",
    "    precisions, recalls, fscores, MAPs, nDCGs = [], [], [], [], []\n",
    "    for k in range(1,11):\n",
    "        precision = evaluator.meanPrecision(\n",
    "            doc_IDs_ordered, query_ids, qrels, k)\n",
    "        precisions.append(precision)\n",
    "        recall = evaluator.meanRecall(\n",
    "            doc_IDs_ordered, query_ids, qrels, k)\n",
    "        recalls.append(recall)\n",
    "        fscore = evaluator.meanFscore(\n",
    "            doc_IDs_ordered, query_ids, qrels, k)\n",
    "        fscores.append(fscore)\n",
    "\n",
    "        MAP = evaluator.meanAveragePrecision(\n",
    "            doc_IDs_ordered, query_ids, qrels, k)\n",
    "        MAPs.append(MAP)\n",
    "        nDCG = evaluator.meanNDCG(\n",
    "            doc_IDs_ordered, query_ids, qrels, k)\n",
    "        nDCGs.append(nDCG)\n",
    "        if (verbose):\n",
    "            print(\"Precision, Recall and F-score @ \" +  \n",
    "                str(k) + \" : \" + str(precision) + \", \" + str(recall) + \n",
    "                \", \" + str(fscore))\n",
    "            print(\"MAP, nDCG @ \" +  \n",
    "                str(k) + \" : \" + str(MAP) + \", \" + str(nDCG))\n",
    "        if (save_results > 0):\n",
    "        # saving the results\n",
    "            with open(op_folder+'Results/LSA_'+str(n_comp)+'.txt', 'a') as f:\n",
    "                f.write(str(k) + \" , \" + str(precision) + \", \" + str(recall) + \n",
    "                        \", \" + str(fscore)+\", \"+str(MAP) + \", \" + str(nDCG)+'\\n')\n",
    "            with open(op_folder+'Results/metrics_'+str(k)+'.txt', 'a') as f:\n",
    "                f.write(str(n_comp) + \" , \" + str(precision) + \", \" + str(recall) + \n",
    "                        \", \" + str(fscore)+\", \"+str(MAP) + \", \" + str(nDCG)+'\\n')\n",
    "            \n",
    "    # Plot the metrics and save plot \n",
    "    if (save_results > 1):\n",
    "        plt.figure()\n",
    "        plt.plot(range(1, 11), precisions, label=\"Precision\")\n",
    "        plt.plot(range(1, 11), recalls, label=\"Recall\")\n",
    "        plt.plot(range(1, 11), fscores, label=\"F-Score\")\n",
    "        plt.plot(range(1, 11), MAPs, label=\"MAP\")\n",
    "        plt.plot(range(1, 11), nDCGs, label=\"nDCG\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Evaluation Metrics - LSA \"+str(n_comp))\n",
    "        plt.xlabel(\"k\")\n",
    "        plt.savefig(op_folder + \"Plots/LSA_\"+str(n_comp)+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of docs and queries included are 1625 and Vocabulary size is 1648\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_1</th>\n",
       "      <th>doc_2</th>\n",
       "      <th>doc_3</th>\n",
       "      <th>doc_4</th>\n",
       "      <th>doc_5</th>\n",
       "      <th>doc_6</th>\n",
       "      <th>doc_7</th>\n",
       "      <th>doc_8</th>\n",
       "      <th>doc_9</th>\n",
       "      <th>doc_10</th>\n",
       "      <th>...</th>\n",
       "      <th>doc_1616</th>\n",
       "      <th>doc_1617</th>\n",
       "      <th>doc_1618</th>\n",
       "      <th>doc_1619</th>\n",
       "      <th>doc_1620</th>\n",
       "      <th>doc_1621</th>\n",
       "      <th>doc_1622</th>\n",
       "      <th>doc_1623</th>\n",
       "      <th>doc_1624</th>\n",
       "      <th>doc_1625</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ablate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ablating</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ablation</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ablative</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absence</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.605634</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1625 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          doc_1  doc_2  doc_3  doc_4  doc_5  doc_6  doc_7  doc_8  doc_9  \\\n",
       "ablate      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "ablating    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "ablation    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "ablative    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "absence     0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "          doc_10  ...  doc_1616  doc_1617  doc_1618  doc_1619  doc_1620  \\\n",
       "ablate       0.0  ...       0.0       0.0  0.000000       0.0       0.0   \n",
       "ablating     0.0  ...       0.0       0.0  0.000000       0.0       0.0   \n",
       "ablation     0.0  ...       0.0       0.0  0.000000       0.0       0.0   \n",
       "ablative     0.0  ...       0.0       0.0  0.000000       0.0       0.0   \n",
       "absence      0.0  ...       0.0       0.0  0.605634       0.0       0.0   \n",
       "\n",
       "          doc_1621  doc_1622  doc_1623  doc_1624  doc_1625  \n",
       "ablate         0.0       0.0       0.0       0.0       0.0  \n",
       "ablating       0.0       0.0       0.0       0.0       0.0  \n",
       "ablation       0.0       0.0       0.0       0.0       0.0  \n",
       "ablative       0.0       0.0       0.0       0.0       0.0  \n",
       "absence        0.0       0.0       0.0       0.0       0.0  \n",
       "\n",
       "[5 rows x 1625 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(\"Total number of docs and queries included are {} and Vocabulary size is {}\".format(X.shape[0],X.shape[1]))\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "columns = ['doc_' + str(i) for i in range(1,X.shape[0]+1)]\n",
    "\n",
    "tfidf_df = pd.DataFrame(X.T.todense(), index=feature_names, columns=columns)\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_docs = tfidf_df[columns[:len(docs_df)]].values\n",
    "vec_rep_queries = tfidf_df[columns[len(docs_df):]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1648, 225)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_rep_queries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = np.matmul(tf_idf_docs.T, vec_rep_queries )\n",
    "doc_IDs_ordered = (np.argsort(cosine_sim,axis=0)+1)[::-1].T.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision, Recall and F-score @ 1 : 0.5911111111111111, 0.09802920317566409, 0.16210530863472036\n",
      "MAP, nDCG @ 1 : 0.5911111111111111, 0.4118518518518518\n",
      "Precision, Recall and F-score @ 2 : 0.4822222222222222, 0.15735581303439025, 0.22459347213488212\n",
      "MAP, nDCG @ 2 : 0.6466666666666666, 0.33368226373889265\n",
      "Precision, Recall and F-score @ 3 : 0.4414814814814813, 0.20782118034681657, 0.2651499335857847\n",
      "MAP, nDCG @ 3 : 0.6614814814814814, 0.32825103615919604\n",
      "Precision, Recall and F-score @ 4 : 0.3877777777777778, 0.23854025406062707, 0.2759486966876725\n",
      "MAP, nDCG @ 4 : 0.6591358024691358, 0.3247484320221337\n",
      "Precision, Recall and F-score @ 5 : 0.35288888888888925, 0.2651030491116788, 0.2816126363966719\n",
      "MAP, nDCG @ 5 : 0.6521111111111109, 0.326260758367758\n",
      "Precision, Recall and F-score @ 6 : 0.317037037037037, 0.28248630120339324, 0.27785442945229755\n",
      "MAP, nDCG @ 6 : 0.6453419753086421, 0.3262984976408044\n",
      "Precision, Recall and F-score @ 7 : 0.2946031746031749, 0.3031342276311141, 0.27791065459632247\n",
      "MAP, nDCG @ 7 : 0.6344432098765429, 0.33093231472956225\n",
      "Precision, Recall and F-score @ 8 : 0.2733333333333333, 0.31612722003135174, 0.2723778383704044\n",
      "MAP, nDCG @ 8 : 0.6295079365079365, 0.33370811890897684\n",
      "Precision, Recall and F-score @ 9 : 0.25283950617283973, 0.3277163654538305, 0.2653638584112871\n",
      "MAP, nDCG @ 9 : 0.6181443436633913, 0.3368600061291843\n",
      "Precision, Recall and F-score @ 10 : 0.2400000000000001, 0.3424461321783339, 0.26270840313601523\n",
      "MAP, nDCG @ 10 : 0.6095962543041908, 0.3411466423633827\n"
     ]
    }
   ],
   "source": [
    "Evaluation_metrics(doc_IDs_ordered, query_ids, qrels,n_comp = 0,save_results=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_corpus_docs = pd.read_csv('./New Corpus/Brown_Corpus_Extracted.csv')\n",
    "brown_corpus_docs['preprocessed'] = brown_corpus_docs[\"docs\"].str.lower()\n",
    "brown_corpus_docs['preprocessed'] = brown_corpus_docs['preprocessed'].apply(clean_text)\n",
    "brown_corpus_docs['preprocessed'] = brown_corpus_docs['preprocessed'].apply(lambda text: remove_punctuation(text))\n",
    "brown_corpus_docs['preprocessed'] = brown_corpus_docs['preprocessed'].apply(lambda text: remove_stopwords(text))\n",
    "brown_corpus_docs['preprocessed'] = brown_corpus_docs['preprocessed'].apply(lambda text: lemmatize_words(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_corpus = brown_corpus_docs['preprocessed'].tolist()\n",
    "total_corpus =  docs_df['preprocessed'].tolist() + query_df['preprocessed'].tolist() + new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(total_corpus)\n",
    "print(\"Total number of docs and queries included are {} and Vocabulary size is {}\".format(X.shape[0],X.shape[1]))\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "columns = ['doc_' + str(i) for i in range(1,X.shape[0]+1)]\n",
    "\n",
    "tfidf_df = pd.DataFrame(X.T.todense(), index=feature_names, columns=columns)\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_docs = tfidf_df[columns[:len(docs_df['preprocessed'].tolist())]].values\n",
    "vec_rep_queries = tfidf_df[columns[len(docs_df['preprocessed'].tolist()):len(docs_df['preprocessed'].tolist()) + \\\n",
    "                                   len(query_df['preprocessed'].tolist())]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cosine_sim = np.matmul(tf_idf_docs.T, vec_rep_queries )\n",
    "doc_IDs_ordered = (np.argsort(cosine_sim,axis=0)+1)[::-1].T.tolist()\n",
    "Evaluation_metrics(doc_IDs_ordered, query_ids, qrels,n_comp = 0,save_results=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA without brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = docs_df['preprocessed'].tolist() + query_df['preprocessed'].tolist()\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(\"Total number of docs and queries included are {} and Vocabulary size is {}\".format(X.shape[0],X.shape[1]))\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "columns = ['doc_' + str(i) for i in range(1,X.shape[0]+1)]\n",
    "\n",
    "tfidf_df = pd.DataFrame(X.T.todense(), index=feature_names, columns=columns)\n",
    "#tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_docs = tfidf_df[columns[:len(docs_df)]].values\n",
    "vec_rep_queries = tfidf_df[columns[len(docs_df):]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_used = [500]\n",
    "for n_comp in components_used:\n",
    "    svd = TruncatedSVD(n_components=n_comp)\n",
    "    svd.fit(tf_idf_docs.T)\n",
    "    tr_docs = svd.transform(tf_idf_docs.T).T\n",
    "    qr_tr = svd.transform(vec_rep_queries.T).T\n",
    "    cosine_sim = np.matmul(tr_docs.T, qr_tr )\n",
    "    doc_IDs_ordered = (np.argsort(cosine_sim,axis=0)+1)[::-1].T.tolist()\n",
    "    #print(\"\\nLSA with \"+str(n_comp)+\" in progress\\n\")\n",
    "    #Evaluation_metrics(doc_IDs_ordered, query_ids, qrels,n_comp = 0,save_results=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_num = 3\n",
    "aaa = true_doc_Ids[true_doc_Ids['query_num'] == query_num]['id'].values.tolist()\n",
    "bbb = doc_IDs_ordered[query_num-1]\n",
    "average_prec = 0\n",
    "for k in range(1,11):\n",
    "    average_prec +=queryPrecision(bbb,aaa,k)/10\n",
    "print('Average Precision : ',average_prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbb[:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA with brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_corpus = brown_corpus_docs['preprocessed'].tolist()\n",
    "total_corpus =  docs_df['preprocessed'].tolist() + query_df['preprocessed'].tolist() + new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(total_corpus)\n",
    "print(\"Total number of docs and queries included are {} and Vocabulary size is {}\".format(X.shape[0],X.shape[1]))\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "columns = ['doc_' + str(i) for i in range(1,X.shape[0]+1)]\n",
    "\n",
    "tfidf_df = pd.DataFrame(X.T.todense(), index=feature_names, columns=columns)\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_docs = tfidf_df[columns[:len(docs_df['preprocessed'].tolist())]].values\n",
    "vec_rep_queries = tfidf_df[columns[len(docs_df['preprocessed'].tolist()):len(docs_df['preprocessed'].tolist()) + \\\n",
    "                                   len(query_df['preprocessed'].tolist())]].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_used = [20, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "for n_comp in components_used:\n",
    "    svd = TruncatedSVD(n_components=n_comp)\n",
    "    svd.fit(tf_idf_docs.T)\n",
    "    tr_docs = svd.transform(tf_idf_docs.T).T\n",
    "    qr_tr = svd.transform(vec_rep_queries.T).T\n",
    "    cosine_sim = np.matmul(tr_docs.T, qr_tr )\n",
    "    doc_IDs_ordered = (np.argsort(cosine_sim,axis=0)+1)[::-1].T.tolist()\n",
    "    print(\"\\nLSA with \"+str(n_comp)+\" in progress\\n\")\n",
    "    Evaluation_metrics(doc_IDs_ordered, query_ids, qrels,n_comp = 0,save_results=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
