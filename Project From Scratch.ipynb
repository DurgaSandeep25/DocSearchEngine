{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from util import *\n",
    "import time\n",
    "from evaluation import Evaluation\n",
    "import string\n",
    "#from gensim.parsing.preprocessing import STOPWORDS\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read queries\n",
    "queries_json = json.load(open( \".\\cranfield\\cran_queries.json\", 'r'))[:]\n",
    "query_ids, queries = [item[\"query number\"] for item in queries_json], \\\n",
    "                        [item[\"query\"] for item in queries_json]\n",
    "\n",
    "# Read documents\n",
    "docs_json = json.load(open(\".\\cranfield\\cran_docs.json\", 'r'))[:]\n",
    "doc_ids, docs = [item[\"id\"] for item in docs_json], \\\n",
    "                        [item[\"body\"] for item in docs_json]\n",
    "\n",
    "qrels = json.load(open( \"./cranfield/cran_qrels.json\", 'r'))[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>experimental investigation of the aerodynamics...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>simple shear flow past a flat plate in an inco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the boundary layer in simple shear flow past a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>approximate solutions of the incompressible la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>one-dimensional transient heat conduction into...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body\n",
       "0  experimental investigation of the aerodynamics...\n",
       "1  simple shear flow past a flat plate in an inco...\n",
       "2  the boundary layer in simple shear flow past a...\n",
       "3  approximate solutions of the incompressible la...\n",
       "4  one-dimensional transient heat conduction into..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df = pd.DataFrame(docs_json).drop(['author','bibliography','id','title'],axis=1)\n",
    "docs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    import re\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"you'll\", \"you will\", text)\n",
    "    text = re.sub(r\"i'll\", \"i will\", text)\n",
    "    text = re.sub(r\"she'll\", \"she will\", text)\n",
    "    text = re.sub(r\"he'll\", \"he will\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"there's\", \"there is\", text)\n",
    "    text = re.sub(r\"here's\", \"here is\", text)\n",
    "    text = re.sub(r\"who's\", \"who is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"shouldn't\", \"should not\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"[^a-z]\", \" \", text) # This removes anything other than lower case letters(very imp)\n",
    "    text = re.sub(r\"   \", \" \", text) # Remove any extra spaces\n",
    "    return text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"custom function to remove the punctuation\"\"\"\n",
    "    PUNCT_TO_REMOVE = string.punctuation + '“' + '”'+'’' + '_'\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df = pd.DataFrame(queries,columns = ['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df['preprocessed'] = docs_df['body'].str.lower()\n",
    "query_df['preprocessed'] = query_df['query'].str.lower()\n",
    "\n",
    "docs_df['preprocessed'] = docs_df['preprocessed'].apply(clean_text)\n",
    "query_df['preprocessed'] = query_df['preprocessed'].apply(clean_text)\n",
    "\n",
    "docs_df['preprocessed'] = docs_df['preprocessed'].apply(lambda text: remove_punctuation(text))\n",
    "query_df['preprocessed'] = query_df['preprocessed'].apply(lambda text: remove_punctuation(text))\n",
    "\n",
    "docs_df['preprocessed'] = docs_df['preprocessed'].apply(lambda text: remove_stopwords(text))\n",
    "query_df['preprocessed'] = query_df['preprocessed'].apply(lambda text: remove_stopwords(text))\n",
    "\n",
    "docs_df['preprocessed'] = docs_df['preprocessed'].apply(lambda text: lemmatize_words(text))\n",
    "query_df['preprocessed'] = query_df['preprocessed'].apply(lambda text: lemmatize_words(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>experimental investigation of the aerodynamics...</td>\n",
       "      <td>experimental investigation aerodynamics wing s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>simple shear flow past a flat plate in an inco...</td>\n",
       "      <td>simple shear flow past flat plate incompressib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the boundary layer in simple shear flow past a...</td>\n",
       "      <td>boundary layer simple shear flow past flat pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>approximate solutions of the incompressible la...</td>\n",
       "      <td>approximate solution incompressible laminar bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>one-dimensional transient heat conduction into...</td>\n",
       "      <td>one dimensional transient heat conduction doub...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  \\\n",
       "0  experimental investigation of the aerodynamics...   \n",
       "1  simple shear flow past a flat plate in an inco...   \n",
       "2  the boundary layer in simple shear flow past a...   \n",
       "3  approximate solutions of the incompressible la...   \n",
       "4  one-dimensional transient heat conduction into...   \n",
       "\n",
       "                                        preprocessed  \n",
       "0  experimental investigation aerodynamics wing s...  \n",
       "1  simple shear flow past flat plate incompressib...  \n",
       "2  boundary layer simple shear flow past flat pla...  \n",
       "3  approximate solution incompressible laminar bo...  \n",
       "4  one dimensional transient heat conduction doub...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = docs_df['preprocessed'].tolist() + query_df['preprocessed'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluation()\n",
    "def Evaluation_metrics(doc_IDs_ordered, query_ids, qrels, n_comp, op_folder = './',save_results = 2, verbose = 1):\n",
    "    \"\"\"\n",
    "    save_results : 0    ===> don't save anything\n",
    "                 : 1    ===> just save results\n",
    "                 : > 2  ===> save plots also\n",
    "    \"\"\"\n",
    "    precisions, recalls, fscores, MAPs, nDCGs = [], [], [], [], []\n",
    "    for k in range(1,11):\n",
    "        precision = evaluator.meanPrecision(\n",
    "            doc_IDs_ordered, query_ids, qrels, k)\n",
    "        precisions.append(precision)\n",
    "        recall = evaluator.meanRecall(\n",
    "            doc_IDs_ordered, query_ids, qrels, k)\n",
    "        recalls.append(recall)\n",
    "        fscore = evaluator.meanFscore(\n",
    "            doc_IDs_ordered, query_ids, qrels, k)\n",
    "        fscores.append(fscore)\n",
    "\n",
    "        MAP = evaluator.meanAveragePrecision(\n",
    "            doc_IDs_ordered, query_ids, qrels, k)\n",
    "        MAPs.append(MAP)\n",
    "        nDCG = evaluator.meanNDCG(\n",
    "            doc_IDs_ordered, query_ids, qrels, k)\n",
    "        nDCGs.append(nDCG)\n",
    "        if (verbose):\n",
    "            print(\"Precision, Recall and F-score @ \" +  \n",
    "                str(k) + \" : \" + str(precision) + \", \" + str(recall) + \n",
    "                \", \" + str(fscore))\n",
    "            print(\"MAP, nDCG @ \" +  \n",
    "                str(k) + \" : \" + str(MAP) + \", \" + str(nDCG))\n",
    "        if (save_results > 0):\n",
    "        # saving the results\n",
    "            with open(op_folder+'Results/LSA_'+str(n_comp)+'.txt', 'a') as f:\n",
    "                f.write(str(k) + \" , \" + str(precision) + \", \" + str(recall) + \n",
    "                        \", \" + str(fscore)+\", \"+str(MAP) + \", \" + str(nDCG)+'\\n')\n",
    "            with open(op_folder+'Results/metrics_'+str(k)+'.txt', 'a') as f:\n",
    "                f.write(str(n_comp) + \" , \" + str(precision) + \", \" + str(recall) + \n",
    "                        \", \" + str(fscore)+\", \"+str(MAP) + \", \" + str(nDCG)+'\\n')\n",
    "            \n",
    "    # Plot the metrics and save plot \n",
    "    if (save_results > 1):\n",
    "        plt.figure()\n",
    "        plt.plot(range(1, 11), precisions, label=\"Precision\")\n",
    "        plt.plot(range(1, 11), recalls, label=\"Recall\")\n",
    "        plt.plot(range(1, 11), fscores, label=\"F-Score\")\n",
    "        plt.plot(range(1, 11), MAPs, label=\"MAP\")\n",
    "        plt.plot(range(1, 11), nDCGs, label=\"nDCG\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Evaluation Metrics - LSA \"+str(n_comp))\n",
    "        plt.xlabel(\"k\")\n",
    "        plt.savefig(op_folder + \"Plots/LSA_\"+str(n_comp)+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "TfidfVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of docs and queries included are 1625 and Vocabulary size is 74831\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_1</th>\n",
       "      <th>doc_2</th>\n",
       "      <th>doc_3</th>\n",
       "      <th>doc_4</th>\n",
       "      <th>doc_5</th>\n",
       "      <th>doc_6</th>\n",
       "      <th>doc_7</th>\n",
       "      <th>doc_8</th>\n",
       "      <th>doc_9</th>\n",
       "      <th>doc_10</th>\n",
       "      <th>...</th>\n",
       "      <th>doc_1616</th>\n",
       "      <th>doc_1617</th>\n",
       "      <th>doc_1618</th>\n",
       "      <th>doc_1619</th>\n",
       "      <th>doc_1620</th>\n",
       "      <th>doc_1621</th>\n",
       "      <th>doc_1622</th>\n",
       "      <th>doc_1623</th>\n",
       "      <th>doc_1624</th>\n",
       "      <th>doc_1625</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ab corresponds</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbreviated form</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ability convert</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ability evaluate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ability structure</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1625 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   doc_1  doc_2  doc_3  doc_4  doc_5  doc_6  doc_7  doc_8  \\\n",
       "ab corresponds       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "abbreviated form     0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "ability convert      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "ability evaluate     0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "ability structure    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "                   doc_9  doc_10  ...  doc_1616  doc_1617  doc_1618  doc_1619  \\\n",
       "ab corresponds       0.0     0.0  ...       0.0       0.0       0.0       0.0   \n",
       "abbreviated form     0.0     0.0  ...       0.0       0.0       0.0       0.0   \n",
       "ability convert      0.0     0.0  ...       0.0       0.0       0.0       0.0   \n",
       "ability evaluate     0.0     0.0  ...       0.0       0.0       0.0       0.0   \n",
       "ability structure    0.0     0.0  ...       0.0       0.0       0.0       0.0   \n",
       "\n",
       "                   doc_1620  doc_1621  doc_1622  doc_1623  doc_1624  doc_1625  \n",
       "ab corresponds          0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "abbreviated form        0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "ability convert         0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "ability evaluate        0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "ability structure       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "\n",
       "[5 rows x 1625 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range = (2,2))\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(\"Total number of docs and queries included are {} and Vocabulary size is {}\".format(X.shape[0],X.shape[1]))\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "columns = ['doc_' + str(i) for i in range(1,X.shape[0]+1)]\n",
    "\n",
    "tfidf_df = pd.DataFrame(X.T.todense(), index=feature_names, columns=columns)\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74831, 1625)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_docs = tfidf_df[columns[:len(docs_df)]].values\n",
    "vec_rep_queries = tfidf_df[columns[len(docs_df):]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74831, 225)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_rep_queries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = np.matmul(tf_idf_docs.T, vec_rep_queries )\n",
    "doc_IDs_ordered = (np.argsort(cosine_sim,axis=0)+1)[::-1].T.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryPrecision(query_doc_IDs_ordered, true_doc_IDs, k):\n",
    "    \n",
    "    precision = -1\n",
    "    \n",
    "    relevance = np.zeros((len(query_doc_IDs_ordered),1))\n",
    "    for i in range(len(query_doc_IDs_ordered)):\n",
    "        if query_doc_IDs_ordered[i] in true_doc_IDs:\n",
    "            relevance[i] = 1\n",
    "\n",
    "    precision = relevance[:k].sum()/k\n",
    "\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_num</th>\n",
       "      <th>position</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_num  position   id\n",
       "0          1         2  184\n",
       "1          1         2   29\n",
       "2          1         2   31\n",
       "3          1         3   12\n",
       "4          1         3   51"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_doc_Ids = pd.read_json(r'''C:\\Users\\Sandeep's\\Desktop\\NLP\\NLP Project\\cranfield\\cran_qrels.json''')\n",
    "true_doc_Ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(number):\n",
    "    corpus = docs_df['preprocessed'].tolist() + query_df['preprocessed'].tolist()\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    #print(\"Total number of docs and queries included are {} and Vocabulary size is {}\".format(X.shape[0],X.shape[1]))\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    columns = ['doc_' + str(i) for i in range(1,X.shape[0]+1)]\n",
    "\n",
    "    tfidf_df = pd.DataFrame(X.T.todense(), index=feature_names, columns=columns)\n",
    "    \n",
    "    tf_idf_docs = tfidf_df[columns[:len(docs_df)]].values\n",
    "    vec_rep_queries = tfidf_df[columns[len(docs_df):]].values\n",
    "    \n",
    "    components_used = [500]\n",
    "    for n_comp in components_used:\n",
    "        svd = TruncatedSVD(n_components=n_comp)\n",
    "        svd.fit(tf_idf_docs.T)\n",
    "        tr_docs = svd.transform(tf_idf_docs.T).T\n",
    "        qr_tr = svd.transform(vec_rep_queries.T).T\n",
    "        cosine_sim = np.matmul(tr_docs.T, qr_tr )\n",
    "        doc_IDs_ordered = (np.argsort(cosine_sim,axis=0)+1)[::-1].T.tolist()\n",
    "    \n",
    "    query_num = number\n",
    "    aaa = true_doc_Ids[true_doc_Ids['query_num'] == query_num]['id'].values.tolist()\n",
    "    bbb = doc_IDs_ordered[query_num-1]\n",
    "    average_prec = 0\n",
    "    for k in range(1,11):\n",
    "        average_prec +=queryPrecision(bbb,aaa,k)/10\n",
    "    print(\"With LSA \")\n",
    "    print('Average Precision : ',average_prec)\n",
    "    print(\"True Docs : \", aaa)\n",
    "    print(\"Predicted Docs : \", bbb[:len(aaa)])\n",
    "    \n",
    "def run2(number):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    #print(\"Total number of docs and queries included are {} and Vocabulary size is {}\".format(X.shape[0],X.shape[1]))\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    columns = ['doc_' + str(i) for i in range(1,X.shape[0]+1)]\n",
    "\n",
    "    tfidf_df = pd.DataFrame(X.T.todense(), index=feature_names, columns=columns)\n",
    "    \n",
    "    tf_idf_docs = tfidf_df[columns[:len(docs_df)]].values\n",
    "    vec_rep_queries = tfidf_df[columns[len(docs_df):]].values\n",
    "    \n",
    "    cosine_sim = np.matmul(tf_idf_docs.T, vec_rep_queries )\n",
    "    doc_IDs_ordered = (np.argsort(cosine_sim,axis=0)+1)[::-1].T.tolist()\n",
    "    query_num = number\n",
    "    aaa = true_doc_Ids[true_doc_Ids['query_num'] == query_num]['id'].values.tolist()\n",
    "    bbb = doc_IDs_ordered[query_num-1]\n",
    "    average_prec = 0\n",
    "    for k in range(1,11):\n",
    "        average_prec +=queryPrecision(bbb,aaa,k)/10\n",
    "    print(\"Without LSA\")\n",
    "    print('Average Precision : ',average_prec)\n",
    "    print(\"True Docs : \", aaa)\n",
    "    print(\"Predicted Docs : \", bbb[:len(aaa)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query num is 16, 115(0.2574), 127, 202,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given Query is : \n",
      "can the transverse potential flow about a body of revolution be calculated efficiently by an electronic computer .\n",
      "Without LSA\n",
      "Average Precision :  0.3407936507936508\n",
      "True Docs :  [463, 462, 497]\n",
      "Predicted Docs :  [462, 1097, 761]\n",
      "With LSA \n",
      "Average Precision :  0.4857936507936508\n",
      "True Docs :  [463, 462, 497]\n",
      "Predicted Docs :  [462, 463, 1097]\n"
     ]
    }
   ],
   "source": [
    "queries = pd.read_json(r'''C:\\Users\\Sandeep's\\Desktop\\NLP\\NLP Project\\cranfield\\cran_queries.json''')\n",
    "query_num = 15\n",
    "print(\"Given Query is : \")\n",
    "print(queries.iloc[query_num]['query'])\n",
    "run2(query_num)\n",
    "run(query_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision, Recall and F-score @ 1 : 0.5866666666666667, 0.10058501128501127, 0.16536591222865724\n",
      "MAP, nDCG @ 1 : 0.5866666666666667, 0.45481481481481484\n",
      "Precision, Recall and F-score @ 2 : 0.49777777777777776, 0.1599801332683899, 0.22869352394819562\n",
      "MAP, nDCG @ 2 : 0.6466666666666666, 0.36084622456323123\n",
      "Precision, Recall and F-score @ 3 : 0.42222222222222217, 0.20095896439894526, 0.2541865585439782\n",
      "MAP, nDCG @ 3 : 0.6540740740740741, 0.33615230372499894\n",
      "Precision, Recall and F-score @ 4 : 0.37555555555555553, 0.23388413002289393, 0.26747136865488386\n",
      "MAP, nDCG @ 4 : 0.6491358024691358, 0.3335926426830179\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-150-07e9ee9f9fcb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mEvaluation_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_IDs_ordered\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqrels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_comp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msave_results\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-247d91bfa434>\u001b[0m in \u001b[0;36mEvaluation_metrics\u001b[1;34m(doc_IDs_ordered, query_ids, qrels, n_comp, op_folder, save_results, verbose)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         MAP = evaluator.meanAveragePrecision(\n\u001b[1;32m---> 21\u001b[1;33m             doc_IDs_ordered, query_ids, qrels, k)\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mMAPs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMAP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         nDCG = evaluator.meanNDCG(\n",
      "\u001b[1;32m~\\Desktop\\NLP\\NLP Project\\evaluation.py\u001b[0m in \u001b[0;36mmeanAveragePrecision\u001b[1;34m(self, doc_IDs_ordered, query_ids, q_rels, k)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m                 \u001b[1;31m#Fill in code here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m                 \u001b[0mqrels_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_rels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m                         \u001b[0mquery_doc_IDs_ordered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc_IDs_ordered\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sandeep's\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    472\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mis_named_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m                         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m                     \u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m                     \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sandeep's\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m         return _list_of_dict_to_arrays(\n\u001b[1;32m--> 464\u001b[1;33m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoerce_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    465\u001b[0m         )\n\u001b[0;32m    466\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sandeep's\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_list_of_dict_to_arrays\u001b[1;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdicts_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m     return _convert_object_array(\n\u001b[1;32m--> 570\u001b[1;33m         \u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoerce_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m     )\n\u001b[0;32m    572\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sandeep's\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_convert_object_array\u001b[1;34m(content, columns, coerce_float, dtype)\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sandeep's\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sandeep's\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(arr)\u001b[0m\n\u001b[0;32m    586\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_convert_objects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtry_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_cast_to_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.maybe_convert_objects\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sandeep's\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36mfull\u001b[1;34m(shape, fill_value, dtype, order)\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m     \u001b[0mmultiarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'unsafe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Evaluation_metrics(doc_IDs_ordered, query_ids, qrels,n_comp = 0,save_results=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_corpus_docs = pd.read_csv('./New Corpus/Brown_Corpus_Extracted.csv')\n",
    "brown_corpus_docs['preprocessed'] = brown_corpus_docs[\"docs\"].str.lower()\n",
    "brown_corpus_docs['preprocessed'] = brown_corpus_docs['preprocessed'].apply(clean_text)\n",
    "brown_corpus_docs['preprocessed'] = brown_corpus_docs['preprocessed'].apply(lambda text: remove_punctuation(text))\n",
    "brown_corpus_docs['preprocessed'] = brown_corpus_docs['preprocessed'].apply(lambda text: remove_stopwords(text))\n",
    "brown_corpus_docs['preprocessed'] = brown_corpus_docs['preprocessed'].apply(lambda text: lemmatize_words(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_corpus = brown_corpus_docs['preprocessed'].tolist()\n",
    "total_corpus =  docs_df['preprocessed'].tolist() + query_df['preprocessed'].tolist() + new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of docs and queries included are 2125 and Vocabulary size is 35174\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_1</th>\n",
       "      <th>doc_2</th>\n",
       "      <th>doc_3</th>\n",
       "      <th>doc_4</th>\n",
       "      <th>doc_5</th>\n",
       "      <th>doc_6</th>\n",
       "      <th>doc_7</th>\n",
       "      <th>doc_8</th>\n",
       "      <th>doc_9</th>\n",
       "      <th>doc_10</th>\n",
       "      <th>...</th>\n",
       "      <th>doc_2116</th>\n",
       "      <th>doc_2117</th>\n",
       "      <th>doc_2118</th>\n",
       "      <th>doc_2119</th>\n",
       "      <th>doc_2120</th>\n",
       "      <th>doc_2121</th>\n",
       "      <th>doc_2122</th>\n",
       "      <th>doc_2123</th>\n",
       "      <th>doc_2124</th>\n",
       "      <th>doc_2125</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aa</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaa</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaawww</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aab</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aah</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2125 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        doc_1  doc_2  doc_3  doc_4  doc_5  doc_6  doc_7  doc_8  doc_9  doc_10  \\\n",
       "aa        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     0.0   \n",
       "aaa       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     0.0   \n",
       "aaawww    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     0.0   \n",
       "aab       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     0.0   \n",
       "aah       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     0.0   \n",
       "\n",
       "        ...  doc_2116  doc_2117  doc_2118  doc_2119  doc_2120  doc_2121  \\\n",
       "aa      ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "aaa     ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "aaawww  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "aab     ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "aah     ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "        doc_2122  doc_2123  doc_2124  doc_2125  \n",
       "aa           0.0       0.0       0.0       0.0  \n",
       "aaa          0.0       0.0       0.0       0.0  \n",
       "aaawww       0.0       0.0       0.0       0.0  \n",
       "aab          0.0       0.0       0.0       0.0  \n",
       "aah          0.0       0.0       0.0       0.0  \n",
       "\n",
       "[5 rows x 2125 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(total_corpus)\n",
    "print(\"Total number of docs and queries included are {} and Vocabulary size is {}\".format(X.shape[0],X.shape[1]))\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "columns = ['doc_' + str(i) for i in range(1,X.shape[0]+1)]\n",
    "\n",
    "tfidf_df = pd.DataFrame(X.T.todense(), index=feature_names, columns=columns)\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_docs = tfidf_df[columns[:len(docs_df['preprocessed'].tolist())]].values\n",
    "vec_rep_queries = tfidf_df[columns[len(docs_df['preprocessed'].tolist()):len(docs_df['preprocessed'].tolist()) + \\\n",
    "                                   len(query_df['preprocessed'].tolist())]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision, Recall and F-score @ 1 : 0.68, 0.11469949720147925, 0.18793485516574437\n",
      "MAP, nDCG @ 1 : 0.68, 0.517037037037037\n",
      "Precision, Recall and F-score @ 2 : 0.5555555555555556, 0.1809491463303036, 0.25610825912266516\n",
      "MAP, nDCG @ 2 : 0.72, 0.4060708358444486\n",
      "Precision, Recall and F-score @ 3 : 0.5022222222222225, 0.23794555695356745, 0.2999380233125523\n",
      "MAP, nDCG @ 3 : 0.7233333333333335, 0.39042744747760777\n",
      "Precision, Recall and F-score @ 4 : 0.45444444444444443, 0.2840794454821928, 0.3234687634565549\n",
      "MAP, nDCG @ 4 : 0.7117283950617282, 0.3840543067186023\n",
      "Precision, Recall and F-score @ 5 : 0.4115555555555559, 0.3141430140974857, 0.3287286078101979\n",
      "MAP, nDCG @ 5 : 0.7087839506172837, 0.3847481480060724\n",
      "Precision, Recall and F-score @ 6 : 0.3785185185185182, 0.3412518145200115, 0.3309532723449875\n",
      "MAP, nDCG @ 6 : 0.6998814814814813, 0.39140010703424105\n",
      "Precision, Recall and F-score @ 7 : 0.35301587301587345, 0.36603255904781495, 0.331303201990575\n",
      "MAP, nDCG @ 7 : 0.6890758377425041, 0.3977807190413258\n",
      "Precision, Recall and F-score @ 8 : 0.33444444444444443, 0.3913747965985148, 0.33310085071208595\n",
      "MAP, nDCG @ 8 : 0.6752761904761898, 0.4047099270143971\n",
      "Precision, Recall and F-score @ 9 : 0.3116049382716053, 0.40452401860792325, 0.32514454156155465\n",
      "MAP, nDCG @ 9 : 0.6660321113630628, 0.40753948998989153\n",
      "Precision, Recall and F-score @ 10 : 0.2937777777777779, 0.42031081472278964, 0.31970890629611787\n",
      "MAP, nDCG @ 10 : 0.6558414126144279, 0.41082955071720295\n"
     ]
    }
   ],
   "source": [
    "cosine_sim = np.matmul(tf_idf_docs.T, vec_rep_queries )\n",
    "doc_IDs_ordered = (np.argsort(cosine_sim,axis=0)+1)[::-1].T.tolist()\n",
    "Evaluation_metrics(doc_IDs_ordered, query_ids, qrels,n_comp = 0,save_results=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA without brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of docs and queries included are 1625 and Vocabulary size is 5604\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_1</th>\n",
       "      <th>doc_2</th>\n",
       "      <th>doc_3</th>\n",
       "      <th>doc_4</th>\n",
       "      <th>doc_5</th>\n",
       "      <th>doc_6</th>\n",
       "      <th>doc_7</th>\n",
       "      <th>doc_8</th>\n",
       "      <th>doc_9</th>\n",
       "      <th>doc_10</th>\n",
       "      <th>...</th>\n",
       "      <th>doc_1616</th>\n",
       "      <th>doc_1617</th>\n",
       "      <th>doc_1618</th>\n",
       "      <th>doc_1619</th>\n",
       "      <th>doc_1620</th>\n",
       "      <th>doc_1621</th>\n",
       "      <th>doc_1622</th>\n",
       "      <th>doc_1623</th>\n",
       "      <th>doc_1624</th>\n",
       "      <th>doc_1625</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ab</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbreviated</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ability</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ablate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ablation</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1625 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             doc_1  doc_2  doc_3  doc_4  doc_5  doc_6  doc_7  doc_8  doc_9  \\\n",
       "ab             0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "abbreviated    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "ability        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "ablate         0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "ablation       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "             doc_10  ...  doc_1616  doc_1617  doc_1618  doc_1619  doc_1620  \\\n",
       "ab              0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "abbreviated     0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "ability         0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "ablate          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "ablation        0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "             doc_1621  doc_1622  doc_1623  doc_1624  doc_1625  \n",
       "ab                0.0       0.0       0.0       0.0       0.0  \n",
       "abbreviated       0.0       0.0       0.0       0.0       0.0  \n",
       "ability           0.0       0.0       0.0       0.0       0.0  \n",
       "ablate            0.0       0.0       0.0       0.0       0.0  \n",
       "ablation          0.0       0.0       0.0       0.0       0.0  \n",
       "\n",
       "[5 rows x 1625 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = docs_df['preprocessed'].tolist() + query_df['preprocessed'].tolist()\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(\"Total number of docs and queries included are {} and Vocabulary size is {}\".format(X.shape[0],X.shape[1]))\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "columns = ['doc_' + str(i) for i in range(1,X.shape[0]+1)]\n",
    "\n",
    "tfidf_df = pd.DataFrame(X.T.todense(), index=feature_names, columns=columns)\n",
    "#tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_docs = tfidf_df[columns[:len(docs_df)]].values\n",
    "vec_rep_queries = tfidf_df[columns[len(docs_df):]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_used = [500]\n",
    "for n_comp in components_used:\n",
    "    svd = TruncatedSVD(n_components=n_comp)\n",
    "    svd.fit(tf_idf_docs.T)\n",
    "    tr_docs = svd.transform(tf_idf_docs.T).T\n",
    "    qr_tr = svd.transform(vec_rep_queries.T).T\n",
    "    cosine_sim = np.matmul(tr_docs.T, qr_tr )\n",
    "    doc_IDs_ordered = (np.argsort(cosine_sim,axis=0)+1)[::-1].T.tolist()\n",
    "    #print(\"\\nLSA with \"+str(n_comp)+\" in progress\\n\")\n",
    "    #Evaluation_metrics(doc_IDs_ordered, query_ids, qrels,n_comp = 0,save_results=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision :  0.9688888888888888\n"
     ]
    }
   ],
   "source": [
    "query_num = 3\n",
    "aaa = true_doc_Ids[true_doc_Ids['query_num'] == query_num]['id'].values.tolist()\n",
    "bbb = doc_IDs_ordered[query_num-1]\n",
    "average_prec = 0\n",
    "for k in range(1,11):\n",
    "    average_prec +=queryPrecision(bbb,aaa,k)/10\n",
    "print('Average Precision : ',average_prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 90, 91, 119, 144, 181, 399, 485]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[485, 5, 90, 91, 144, 181, 399, 6, 582, 707, 579]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbb[:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA with brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_corpus = brown_corpus_docs['preprocessed'].tolist()\n",
    "total_corpus =  docs_df['preprocessed'].tolist() + query_df['preprocessed'].tolist() + new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of docs and queries included are 2125 and Vocabulary size is 35174\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_1</th>\n",
       "      <th>doc_2</th>\n",
       "      <th>doc_3</th>\n",
       "      <th>doc_4</th>\n",
       "      <th>doc_5</th>\n",
       "      <th>doc_6</th>\n",
       "      <th>doc_7</th>\n",
       "      <th>doc_8</th>\n",
       "      <th>doc_9</th>\n",
       "      <th>doc_10</th>\n",
       "      <th>...</th>\n",
       "      <th>doc_2116</th>\n",
       "      <th>doc_2117</th>\n",
       "      <th>doc_2118</th>\n",
       "      <th>doc_2119</th>\n",
       "      <th>doc_2120</th>\n",
       "      <th>doc_2121</th>\n",
       "      <th>doc_2122</th>\n",
       "      <th>doc_2123</th>\n",
       "      <th>doc_2124</th>\n",
       "      <th>doc_2125</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aa</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaa</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaawww</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aab</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aah</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2125 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        doc_1  doc_2  doc_3  doc_4  doc_5  doc_6  doc_7  doc_8  doc_9  doc_10  \\\n",
       "aa        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     0.0   \n",
       "aaa       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     0.0   \n",
       "aaawww    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     0.0   \n",
       "aab       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     0.0   \n",
       "aah       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     0.0   \n",
       "\n",
       "        ...  doc_2116  doc_2117  doc_2118  doc_2119  doc_2120  doc_2121  \\\n",
       "aa      ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "aaa     ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "aaawww  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "aab     ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "aah     ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "        doc_2122  doc_2123  doc_2124  doc_2125  \n",
       "aa           0.0       0.0       0.0       0.0  \n",
       "aaa          0.0       0.0       0.0       0.0  \n",
       "aaawww       0.0       0.0       0.0       0.0  \n",
       "aab          0.0       0.0       0.0       0.0  \n",
       "aah          0.0       0.0       0.0       0.0  \n",
       "\n",
       "[5 rows x 2125 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(total_corpus)\n",
    "print(\"Total number of docs and queries included are {} and Vocabulary size is {}\".format(X.shape[0],X.shape[1]))\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "columns = ['doc_' + str(i) for i in range(1,X.shape[0]+1)]\n",
    "\n",
    "tfidf_df = pd.DataFrame(X.T.todense(), index=feature_names, columns=columns)\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_docs = tfidf_df[columns[:len(docs_df['preprocessed'].tolist())]].values\n",
    "vec_rep_queries = tfidf_df[columns[len(docs_df['preprocessed'].tolist()):len(docs_df['preprocessed'].tolist()) + \\\n",
    "                                   len(query_df['preprocessed'].tolist())]].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LSA with 20 in progress\n",
      "\n",
      "Precision, Recall and F-score @ 1 : 0.2222222222222222, 0.029681586509656677, 0.05064216963776479\n",
      "MAP, nDCG @ 1 : 0.2222222222222222, 0.12888888888888886\n",
      "Precision, Recall and F-score @ 2 : 0.19555555555555557, 0.05055103577384276, 0.076498848167504\n",
      "MAP, nDCG @ 2 : 0.26222222222222225, 0.10247873896931342\n",
      "Precision, Recall and F-score @ 3 : 0.19407407407407404, 0.07661072916686949, 0.10347668450628146\n",
      "MAP, nDCG @ 3 : 0.28592592592592597, 0.10727330922052095\n",
      "Precision, Recall and F-score @ 4 : 0.18333333333333332, 0.09939878243531489, 0.1202169358967111\n",
      "MAP, nDCG @ 4 : 0.29629629629629634, 0.11024132036007335\n",
      "Precision, Recall and F-score @ 5 : 0.17866666666666658, 0.11881537753230213, 0.1326191564466702\n",
      "MAP, nDCG @ 5 : 0.29373456790123464, 0.1137841294724282\n",
      "Precision, Recall and F-score @ 6 : 0.18222222222222223, 0.1463252777036058, 0.1501491844095761\n",
      "MAP, nDCG @ 6 : 0.29632345679012345, 0.12315144983791576\n",
      "Precision, Recall and F-score @ 7 : 0.17079365079365072, 0.15834748116234182, 0.1519955636255884\n",
      "MAP, nDCG @ 7 : 0.2984596119929453, 0.12665563226005294\n",
      "Precision, Recall and F-score @ 8 : 0.16111111111111112, 0.16942665535464363, 0.15247601796201155\n",
      "MAP, nDCG @ 8 : 0.2996158226253465, 0.13048396990523606\n",
      "Precision, Recall and F-score @ 9 : 0.15950617283950597, 0.1886764500894959, 0.15984454768647402\n",
      "MAP, nDCG @ 9 : 0.2947363567649283, 0.13869574750999636\n",
      "Precision, Recall and F-score @ 10 : 0.15822222222222232, 0.20954102691952245, 0.16679250173044233\n",
      "MAP, nDCG @ 10 : 0.2945756991685565, 0.14566345322243318\n",
      "\n",
      "LSA with 50 in progress\n",
      "\n",
      "Precision, Recall and F-score @ 1 : 0.39111111111111113, 0.05580729679677044, 0.09435024660074368\n",
      "MAP, nDCG @ 1 : 0.39111111111111113, 0.2385185185185186\n",
      "Precision, Recall and F-score @ 2 : 0.34444444444444444, 0.0999781445526757, 0.14642164762540777\n",
      "MAP, nDCG @ 2 : 0.44666666666666666, 0.20226593219047875\n",
      "Precision, Recall and F-score @ 3 : 0.34222222222222204, 0.15169373171472383, 0.19556579190390114\n",
      "MAP, nDCG @ 3 : 0.4662962962962962, 0.21109731790240172\n",
      "Precision, Recall and F-score @ 4 : 0.31333333333333335, 0.18059524814835662, 0.21217566670565305\n",
      "MAP, nDCG @ 4 : 0.47654320987654325, 0.21311838789622078\n",
      "Precision, Recall and F-score @ 5 : 0.2968888888888892, 0.21496755666772394, 0.23032419813127827\n",
      "MAP, nDCG @ 5 : 0.48335185185185203, 0.22232669347300063\n",
      "Precision, Recall and F-score @ 6 : 0.27703703703703697, 0.23563322129883824, 0.23479918488514906\n",
      "MAP, nDCG @ 6 : 0.4819950617283951, 0.23030774159355258\n",
      "Precision, Recall and F-score @ 7 : 0.2653968253968255, 0.25610121836035504, 0.24031953019671798\n",
      "MAP, nDCG @ 7 : 0.4771467372134039, 0.23808553432596666\n",
      "Precision, Recall and F-score @ 8 : 0.2538888888888889, 0.2795257012848379, 0.24592962968708246\n",
      "MAP, nDCG @ 8 : 0.47219188712522053, 0.2449236600410224\n",
      "Precision, Recall and F-score @ 9 : 0.23999999999999996, 0.2949860288065691, 0.24496989272295797\n",
      "MAP, nDCG @ 9 : 0.46926121819098016, 0.24996907104860852\n",
      "Precision, Recall and F-score @ 10 : 0.23200000000000007, 0.31302360188593786, 0.24680023390589642\n",
      "MAP, nDCG @ 10 : 0.46341142045295475, 0.2563885769835976\n",
      "\n",
      "LSA with 100 in progress\n",
      "\n",
      "Precision, Recall and F-score @ 1 : 0.5377777777777778, 0.07942726761552425, 0.13355620912043165\n",
      "MAP, nDCG @ 1 : 0.5377777777777778, 0.3659259259259257\n",
      "Precision, Recall and F-score @ 2 : 0.46444444444444444, 0.14039985116849993, 0.20358870192696488\n",
      "MAP, nDCG @ 2 : 0.6022222222222222, 0.29578655818280764\n",
      "Precision, Recall and F-score @ 3 : 0.4207407407407405, 0.18750120850197355, 0.24199916120199816\n",
      "MAP, nDCG @ 3 : 0.6066666666666662, 0.29074106311350983\n",
      "Precision, Recall and F-score @ 4 : 0.4, 0.2338186158711052, 0.2742446347912473\n",
      "MAP, nDCG @ 4 : 0.605185185185185, 0.29755782125077745\n",
      "Precision, Recall and F-score @ 5 : 0.37333333333333346, 0.2684416963157756, 0.28899420146051796\n",
      "MAP, nDCG @ 5 : 0.598320987654321, 0.3036083179268799\n",
      "Precision, Recall and F-score @ 6 : 0.3481481481481482, 0.299313710153239, 0.29756919231622664\n",
      "MAP, nDCG @ 6 : 0.5935345679012346, 0.31190626130496973\n",
      "Precision, Recall and F-score @ 7 : 0.32888888888888923, 0.3281429382961927, 0.30340978664346485\n",
      "MAP, nDCG @ 7 : 0.5829954144620814, 0.32194016329654307\n",
      "Precision, Recall and F-score @ 8 : 0.3088888888888889, 0.34567359492036914, 0.30143935796412\n",
      "MAP, nDCG @ 8 : 0.5760718568909047, 0.32675291110430554\n",
      "Precision, Recall and F-score @ 9 : 0.2898765432098769, 0.36186866783383287, 0.29715988877970023\n",
      "MAP, nDCG @ 9 : 0.5717787100025195, 0.33239508365726983\n",
      "Precision, Recall and F-score @ 10 : 0.2773333333333334, 0.3791233170832189, 0.2962733754879864\n",
      "MAP, nDCG @ 10 : 0.5653955089443186, 0.3370129433533755\n",
      "\n",
      "LSA with 200 in progress\n",
      "\n",
      "Precision, Recall and F-score @ 1 : 0.6355555555555555, 0.09917844418042629, 0.1650203412120147\n",
      "MAP, nDCG @ 1 : 0.6355555555555555, 0.4518518518518518\n",
      "Precision, Recall and F-score @ 2 : 0.54, 0.16900003494785887, 0.24256595780258614\n",
      "MAP, nDCG @ 2 : 0.6933333333333334, 0.3690645104796342\n",
      "Precision, Recall and F-score @ 3 : 0.4948148148148148, 0.23125716451811615, 0.29346450721171985\n",
      "MAP, nDCG @ 3 : 0.6922222222222222, 0.3629012241163001\n",
      "Precision, Recall and F-score @ 4 : 0.4477777777777778, 0.27241885723153314, 0.3142209114840675\n",
      "MAP, nDCG @ 4 : 0.6893827160493826, 0.3604271867286926\n",
      "Precision, Recall and F-score @ 5 : 0.4151111111111114, 0.3114276054683514, 0.32912516081954785\n",
      "MAP, nDCG @ 5 : 0.6812592592592586, 0.36506754144462844\n",
      "Precision, Recall and F-score @ 6 : 0.382962962962963, 0.34091167526088356, 0.3324987779961033\n",
      "MAP, nDCG @ 6 : 0.6767259259259255, 0.37156678036559315\n",
      "Precision, Recall and F-score @ 7 : 0.3536507936507942, 0.36353674953301685, 0.33085453482191246\n",
      "MAP, nDCG @ 7 : 0.663112698412698, 0.37699901303347105\n",
      "Precision, Recall and F-score @ 8 : 0.3327777777777778, 0.38488062607567636, 0.32965442052620114\n",
      "MAP, nDCG @ 8 : 0.6550108339632148, 0.383507478488603\n",
      "Precision, Recall and F-score @ 9 : 0.31506172839506197, 0.40262961555273585, 0.3270664039307259\n",
      "MAP, nDCG @ 9 : 0.6449516061980345, 0.3880067092072333\n",
      "Precision, Recall and F-score @ 10 : 0.30222222222222245, 0.42352580844366566, 0.3267173398155279\n",
      "MAP, nDCG @ 10 : 0.6328855141233446, 0.3949682851309179\n",
      "\n",
      "LSA with 300 in progress\n",
      "\n",
      "Precision, Recall and F-score @ 1 : 0.6622222222222223, 0.10576106526304734, 0.17535845625601212\n",
      "MAP, nDCG @ 1 : 0.6622222222222223, 0.48592592592592593\n",
      "Precision, Recall and F-score @ 2 : 0.5622222222222222, 0.1802250630062202, 0.2567414613114229\n",
      "MAP, nDCG @ 2 : 0.7088888888888889, 0.3940511572587347\n",
      "Precision, Recall and F-score @ 3 : 0.4977777777777778, 0.2344039472923499, 0.29579338496165014\n",
      "MAP, nDCG @ 3 : 0.7133333333333336, 0.37367030202680046\n",
      "Precision, Recall and F-score @ 4 : 0.4577777777777778, 0.2827982754664724, 0.3234232245573097\n",
      "MAP, nDCG @ 4 : 0.7055555555555557, 0.37431333513246307\n",
      "Precision, Recall and F-score @ 5 : 0.42133333333333356, 0.31953470570290265, 0.33538918983796256\n",
      "MAP, nDCG @ 5 : 0.7009814814814812, 0.37759994880132697\n",
      "Precision, Recall and F-score @ 6 : 0.38370370370370366, 0.34356996115188365, 0.33424937567287305\n",
      "MAP, nDCG @ 6 : 0.6893086419753081, 0.38281226335627394\n",
      "Precision, Recall and F-score @ 7 : 0.35936507936507983, 0.37088291544523017, 0.3366302656201377\n",
      "MAP, nDCG @ 7 : 0.6797686067019394, 0.39066714397559055\n",
      "Precision, Recall and F-score @ 8 : 0.34, 0.39426146853224564, 0.33726425917041775\n",
      "MAP, nDCG @ 8 : 0.670437339380196, 0.3982124570522303\n",
      "Precision, Recall and F-score @ 9 : 0.31703703703703745, 0.40876396376281104, 0.33009917149642665\n",
      "MAP, nDCG @ 9 : 0.658468978332073, 0.40156499020348024\n",
      "Precision, Recall and F-score @ 10 : 0.30088888888888915, 0.42848458265009653, 0.32745621297711847\n",
      "MAP, nDCG @ 10 : 0.6490236534531504, 0.40572364856850274\n",
      "\n",
      "LSA with 400 in progress\n",
      "\n",
      "Precision, Recall and F-score @ 1 : 0.6933333333333334, 0.11372402822601027, 0.18712773635862562\n",
      "MAP, nDCG @ 1 : 0.6933333333333334, 0.5214814814814815\n",
      "Precision, Recall and F-score @ 2 : 0.5733333333333334, 0.18728231933014322, 0.26482247446433144\n",
      "MAP, nDCG @ 2 : 0.7333333333333333, 0.41029917224259677\n",
      "Precision, Recall and F-score @ 3 : 0.5037037037037037, 0.24066440905281175, 0.30230354667807563\n",
      "MAP, nDCG @ 3 : 0.7318518518518521, 0.38787963972527506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision, Recall and F-score @ 4 : 0.45444444444444443, 0.28227135358660943, 0.3218338386345905\n",
      "MAP, nDCG @ 4 : 0.7240740740740742, 0.3869994648376415\n",
      "Precision, Recall and F-score @ 5 : 0.41688888888888914, 0.31672965331157577, 0.33218082758522466\n",
      "MAP, nDCG @ 5 : 0.7165740740740738, 0.387041793307716\n",
      "Precision, Recall and F-score @ 6 : 0.3792592592592592, 0.34001766159958413, 0.3306328724013991\n",
      "MAP, nDCG @ 6 : 0.7071320987654317, 0.39055279808287396\n",
      "Precision, Recall and F-score @ 7 : 0.35873015873015923, 0.37281007905866814, 0.3373849886964982\n",
      "MAP, nDCG @ 7 : 0.6918432098765427, 0.4007053951950559\n",
      "Precision, Recall and F-score @ 8 : 0.33611111111111114, 0.3935193777483591, 0.3350192927171557\n",
      "MAP, nDCG @ 8 : 0.680068027210884, 0.4057154441613624\n",
      "Precision, Recall and F-score @ 9 : 0.3165432098765435, 0.410762457471831, 0.33072392642825954\n",
      "MAP, nDCG @ 9 : 0.6684863378684806, 0.4105727012805075\n",
      "Precision, Recall and F-score @ 10 : 0.2955555555555557, 0.42270200330611374, 0.32211789565724763\n",
      "MAP, nDCG @ 10 : 0.6630315087763499, 0.41350361352766485\n",
      "\n",
      "LSA with 500 in progress\n",
      "\n",
      "Precision, Recall and F-score @ 1 : 0.6933333333333334, 0.11490921341119545, 0.18875351798440723\n",
      "MAP, nDCG @ 1 : 0.6933333333333334, 0.525925925925926\n",
      "Precision, Recall and F-score @ 2 : 0.5711111111111111, 0.18698862926452336, 0.26414407515848115\n",
      "MAP, nDCG @ 2 : 0.7288888888888889, 0.4110735064886285\n",
      "Precision, Recall and F-score @ 3 : 0.5096296296296298, 0.2439780706998068, 0.3063403617148906\n",
      "MAP, nDCG @ 3 : 0.7303703703703704, 0.39160023686292894\n",
      "Precision, Recall and F-score @ 4 : 0.4577777777777778, 0.28538422836615085, 0.3250178398185916\n",
      "MAP, nDCG @ 4 : 0.7276543209876544, 0.39019847181829703\n",
      "Precision, Recall and F-score @ 5 : 0.4133333333333335, 0.31492558084083666, 0.3297144815855454\n",
      "MAP, nDCG @ 5 : 0.7173086419753085, 0.3882267946334299\n",
      "Precision, Recall and F-score @ 6 : 0.37851851851851853, 0.3413933229752455, 0.33112118655637995\n",
      "MAP, nDCG @ 6 : 0.7058271604938267, 0.3915502549463121\n",
      "Precision, Recall and F-score @ 7 : 0.36000000000000043, 0.3762121954607845, 0.33960691583483393\n",
      "MAP, nDCG @ 7 : 0.6910874779541443, 0.4036125125387292\n",
      "Precision, Recall and F-score @ 8 : 0.33444444444444443, 0.3928825331115144, 0.33382973373812297\n",
      "MAP, nDCG @ 8 : 0.6817743764172334, 0.40690227146693314\n",
      "Precision, Recall and F-score @ 9 : 0.31259259259259303, 0.4070736242777347, 0.3269149268050185\n",
      "MAP, nDCG @ 9 : 0.6730534454522548, 0.4115819652616479\n",
      "Precision, Recall and F-score @ 10 : 0.29688888888888915, 0.42648599358484085, 0.32416539955937007\n",
      "MAP, nDCG @ 10 : 0.6581270093222473, 0.4169668549607247\n",
      "\n",
      "LSA with 600 in progress\n",
      "\n",
      "Precision, Recall and F-score @ 1 : 0.6888888888888889, 0.1155388430408251, 0.18943253033008622\n",
      "MAP, nDCG @ 1 : 0.6888888888888889, 0.52\n",
      "Precision, Recall and F-score @ 2 : 0.5688888888888889, 0.1862231971657579, 0.2631955422099482\n",
      "MAP, nDCG @ 2 : 0.7244444444444444, 0.40904525427169725\n",
      "Precision, Recall and F-score @ 3 : 0.5096296296296297, 0.2435389172606533, 0.3059385659797616\n",
      "MAP, nDCG @ 3 : 0.7288888888888889, 0.39052233360468513\n",
      "Precision, Recall and F-score @ 4 : 0.4588888888888889, 0.2855832903664299, 0.3256572329141251\n",
      "MAP, nDCG @ 4 : 0.7277777777777777, 0.38803213044326373\n",
      "Precision, Recall and F-score @ 5 : 0.417777777777778, 0.31796234654426914, 0.3332809398186703\n",
      "MAP, nDCG @ 5 : 0.7174135802469134, 0.390104310840778\n",
      "Precision, Recall and F-score @ 6 : 0.38222222222222224, 0.34373916232108487, 0.33383020133729013\n",
      "MAP, nDCG @ 6 : 0.7048271604938267, 0.3937631750760339\n",
      "Precision, Recall and F-score @ 7 : 0.3561904761904766, 0.37086314944507187, 0.3352877050986985\n",
      "MAP, nDCG @ 7 : 0.694645326278659, 0.4010156920837944\n",
      "Precision, Recall and F-score @ 8 : 0.3377777777777778, 0.3940069113972962, 0.33577146121214874\n",
      "MAP, nDCG @ 8 : 0.6832017132779037, 0.40688663574420997\n",
      "Precision, Recall and F-score @ 9 : 0.31456790123456824, 0.40734014815738606, 0.3281259026513474\n",
      "MAP, nDCG @ 9 : 0.6752618165784832, 0.4108788341099529\n",
      "Precision, Recall and F-score @ 10 : 0.2995555555555557, 0.42610297441494915, 0.3257534404011171\n",
      "MAP, nDCG @ 10 : 0.6623795372469974, 0.4165373917403331\n",
      "\n",
      "LSA with 700 in progress\n",
      "\n",
      "Precision, Recall and F-score @ 1 : 0.6755555555555556, 0.11416318166516373, 0.1869577828553388\n",
      "MAP, nDCG @ 1 : 0.6755555555555556, 0.5081481481481482\n",
      "Precision, Recall and F-score @ 2 : 0.5666666666666667, 0.1850578440004047, 0.26179111947219214\n",
      "MAP, nDCG @ 2 : 0.7177777777777777, 0.4044922285677291\n",
      "Precision, Recall and F-score @ 3 : 0.5081481481481482, 0.24398129405597124, 0.30645179349298907\n",
      "MAP, nDCG @ 3 : 0.7237037037037037, 0.38979022668619345\n",
      "Precision, Recall and F-score @ 4 : 0.4533333333333333, 0.28410533622180906, 0.3230175639411228\n",
      "MAP, nDCG @ 4 : 0.7206172839506173, 0.3833355590922475\n",
      "Precision, Recall and F-score @ 5 : 0.41688888888888914, 0.31905873780732696, 0.3335063687107659\n",
      "MAP, nDCG @ 5 : 0.7080370370370367, 0.3861172565180008\n",
      "Precision, Recall and F-score @ 6 : 0.38592592592592584, 0.34744639336164923, 0.3373669978740867\n",
      "MAP, nDCG @ 6 : 0.6973888888888885, 0.39152092427951724\n",
      "Precision, Recall and F-score @ 7 : 0.35301587301587345, 0.3691753187572412, 0.3331617114886129\n",
      "MAP, nDCG @ 7 : 0.6913763668430332, 0.39732977496779265\n",
      "Precision, Recall and F-score @ 8 : 0.3327777777777778, 0.3881689368312515, 0.33081410956397583\n",
      "MAP, nDCG @ 8 : 0.678469841269841, 0.40297352637254186\n",
      "Precision, Recall and F-score @ 9 : 0.31555555555555587, 0.40850064198454655, 0.3292950710334501\n",
      "MAP, nDCG @ 9 : 0.6663664965986391, 0.4089766993276929\n",
      "Precision, Recall and F-score @ 10 : 0.29600000000000015, 0.42379161520885317, 0.3228957421205525\n",
      "MAP, nDCG @ 10 : 0.6584515096161918, 0.4126568114594948\n",
      "\n",
      "LSA with 800 in progress\n",
      "\n",
      "Precision, Recall and F-score @ 1 : 0.6755555555555556, 0.11323725573923779, 0.18572321495410418\n",
      "MAP, nDCG @ 1 : 0.6755555555555556, 0.5081481481481482\n",
      "Precision, Recall and F-score @ 2 : 0.5555555555555556, 0.18084620378876448, 0.25593044927818864\n",
      "MAP, nDCG @ 2 : 0.7111111111111111, 0.40274804429524225\n",
      "Precision, Recall and F-score @ 3 : 0.5081481481481483, 0.2422898250311689, 0.3048092755171378\n",
      "MAP, nDCG @ 3 : 0.7181481481481483, 0.3895911263067339\n",
      "Precision, Recall and F-score @ 4 : 0.45666666666666667, 0.28520721400996135, 0.32493567052589606\n",
      "MAP, nDCG @ 4 : 0.7201234567901236, 0.38406693267019815\n",
      "Precision, Recall and F-score @ 5 : 0.4168888888888892, 0.3180302477788369, 0.3327866933244237\n",
      "MAP, nDCG @ 5 : 0.7044444444444443, 0.38663370039874034\n",
      "Precision, Recall and F-score @ 6 : 0.3814814814814813, 0.34285364043556293, 0.33295974717271837\n",
      "MAP, nDCG @ 6 : 0.6985419753086417, 0.39072087541355244\n",
      "Precision, Recall and F-score @ 7 : 0.35428571428571476, 0.36942709167568083, 0.33407622544234256\n",
      "MAP, nDCG @ 7 : 0.6871742504409166, 0.39653428588565737\n",
      "Precision, Recall and F-score @ 8 : 0.3333333333333333, 0.38895047400752547, 0.3314960613570387\n",
      "MAP, nDCG @ 8 : 0.6751144620811282, 0.402687332544539\n",
      "Precision, Recall and F-score @ 9 : 0.31209876543209913, 0.40443891358948486, 0.3257027303934904\n",
      "MAP, nDCG @ 9 : 0.6661081254724107, 0.4072614395327131\n",
      "Precision, Recall and F-score @ 10 : 0.2946666666666669, 0.4224147032319413, 0.3214701212261144\n",
      "MAP, nDCG @ 10 : 0.656894205089443, 0.4114532641707359\n",
      "\n",
      "LSA with 900 in progress\n",
      "\n",
      "Precision, Recall and F-score @ 1 : 0.68, 0.11379281129479335, 0.18671086927509187\n",
      "MAP, nDCG @ 1 : 0.68, 0.5140740740740741\n",
      "Precision, Recall and F-score @ 2 : 0.56, 0.1826451455877063, 0.258263782611522\n",
      "MAP, nDCG @ 2 : 0.7177777777777777, 0.40740142813730584\n",
      "Precision, Recall and F-score @ 3 : 0.5081481481481482, 0.24287183561317943, 0.305288993330189\n",
      "MAP, nDCG @ 3 : 0.7237037037037037, 0.3915868646290666\n",
      "Precision, Recall and F-score @ 4 : 0.45666666666666667, 0.2842206992901133, 0.3239796846341428\n",
      "MAP, nDCG @ 4 : 0.7192592592592593, 0.38448309416931015\n",
      "Precision, Recall and F-score @ 5 : 0.40888888888888913, 0.3121641781186496, 0.3264955893666532\n",
      "MAP, nDCG @ 5 : 0.7081049382716048, 0.3824933307979742\n",
      "Precision, Recall and F-score @ 6 : 0.3807407407407405, 0.34315875507401095, 0.33302065745585074\n",
      "MAP, nDCG @ 6 : 0.6975419753086416, 0.39074484236223594\n",
      "Precision, Recall and F-score @ 7 : 0.35365079365079416, 0.3678138641624534, 0.3328318511381501\n",
      "MAP, nDCG @ 7 : 0.6899171075837738, 0.39721683011096376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision, Recall and F-score @ 8 : 0.3322222222222222, 0.3887540570391787, 0.3306944887492711\n",
      "MAP, nDCG @ 8 : 0.6745015873015865, 0.4030193910126372\n",
      "Precision, Recall and F-score @ 9 : 0.31111111111111134, 0.4036597149252288, 0.3247102434501951\n",
      "MAP, nDCG @ 9 : 0.6639905769715287, 0.40739808835530494\n",
      "Precision, Recall and F-score @ 10 : 0.2951111111111114, 0.42345226936950736, 0.3219099602541888\n",
      "MAP, nDCG @ 10 : 0.656919131603258, 0.41259923374088187\n",
      "\n",
      "LSA with 1000 in progress\n",
      "\n",
      "Precision, Recall and F-score @ 1 : 0.6844444444444444, 0.11620021870220075, 0.19016765939854866\n",
      "MAP, nDCG @ 1 : 0.6844444444444444, 0.5155555555555557\n",
      "Precision, Recall and F-score @ 2 : 0.56, 0.1832359745118686, 0.25902360303800903\n",
      "MAP, nDCG @ 2 : 0.72, 0.4086871122531789\n",
      "Precision, Recall and F-score @ 3 : 0.5051851851851853, 0.24283656224457276, 0.30456941661061226\n",
      "MAP, nDCG @ 3 : 0.7255555555555557, 0.39108102752762924\n",
      "Precision, Recall and F-score @ 4 : 0.45555555555555555, 0.28467246974188376, 0.3240740927677666\n",
      "MAP, nDCG @ 4 : 0.7214814814814816, 0.38439613244998644\n",
      "Precision, Recall and F-score @ 5 : 0.4071111111111115, 0.30986186931634074, 0.3244948045763946\n",
      "MAP, nDCG @ 5 : 0.7123765432098764, 0.3829920245049175\n",
      "Precision, Recall and F-score @ 6 : 0.3807407407407405, 0.3431164270316829, 0.33294536238055583\n",
      "MAP, nDCG @ 6 : 0.6991222222222219, 0.39182323748976616\n",
      "Precision, Recall and F-score @ 7 : 0.3542857142857147, 0.36761404052929636, 0.33292037306426786\n",
      "MAP, nDCG @ 7 : 0.6910075837742501, 0.3982238978352004\n",
      "Precision, Recall and F-score @ 8 : 0.33555555555555555, 0.39127872628223875, 0.3334316139033208\n",
      "MAP, nDCG @ 8 : 0.6726659611992941, 0.4052812324092341\n",
      "Precision, Recall and F-score @ 9 : 0.3111111111111113, 0.405467292617864, 0.32522126386841105\n",
      "MAP, nDCG @ 9 : 0.667625938523557, 0.40828056297160753\n",
      "Precision, Recall and F-score @ 10 : 0.2955555555555558, 0.42329001187391657, 0.3221647812043657\n",
      "MAP, nDCG @ 10 : 0.6564006130847396, 0.41368805513717494\n"
     ]
    }
   ],
   "source": [
    "components_used = [20, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "for n_comp in components_used:\n",
    "    svd = TruncatedSVD(n_components=n_comp)\n",
    "    svd.fit(tf_idf_docs.T)\n",
    "    tr_docs = svd.transform(tf_idf_docs.T).T\n",
    "    qr_tr = svd.transform(vec_rep_queries.T).T\n",
    "    cosine_sim = np.matmul(tr_docs.T, qr_tr )\n",
    "    doc_IDs_ordered = (np.argsort(cosine_sim,axis=0)+1)[::-1].T.tolist()\n",
    "    print(\"\\nLSA with \"+str(n_comp)+\" in progress\\n\")\n",
    "    Evaluation_metrics(doc_IDs_ordered, query_ids, qrels,n_comp = 0,save_results=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
