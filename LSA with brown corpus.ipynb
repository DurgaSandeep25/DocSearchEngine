{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentenceSegmentation import SentenceSegmentation\n",
    "from tokenization import Tokenization\n",
    "from inflectionReduction import InflectionReduction\n",
    "from stopwordRemoval import StopwordRemoval\n",
    "from informationRetrieval import InformationRetrieval\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sys import version_info\n",
    "# import argparse\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from util import *\n",
    "# from main import SearchEngine\n",
    "from evaluation import Evaluation\n",
    "import string\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data importing and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_corpus_docs = pd.read_csv('./New Corpus/Brown_Corpus_Extracted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cd05</td>\n",
       "      <td>Furthermore , as an encouragement to revisioni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cf37</td>\n",
       "      <td>The missionary obligation to proclaim the gosp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cj50</td>\n",
       "      <td>Unfortunately , however , and for reasons to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cf08</td>\n",
       "      <td>In tradition and in poetry , the marriage bed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cl06</td>\n",
       "      <td>Eight , nine steps above him , Roberts had pau...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  filename                                               docs\n",
       "0     cd05  Furthermore , as an encouragement to revisioni...\n",
       "1     cf37  The missionary obligation to proclaim the gosp...\n",
       "2     cj50  Unfortunately , however , and for reasons to b...\n",
       "3     cf08  In tradition and in poetry , the marriage bed ...\n",
       "4     cl06  Eight , nine steps above him , Roberts had pau..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_corpus_docs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_corpus_docs['preprocessed'] = brown_corpus_docs[\"docs\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = SentenceSegmentation()\n",
    "p2 = Tokenization()\n",
    "p3 = InflectionReduction()\n",
    "p4 = StopwordRemoval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_corpus_docs['preprocessed'] = brown_corpus_docs['preprocessed'].apply(lambda x : p1.punkt(x))\n",
    "brown_corpus_docs['preprocessed'] = brown_corpus_docs['preprocessed'].apply(lambda x : p2.pennTreeBank(x))\n",
    "brown_corpus_docs['preprocessed'] = brown_corpus_docs['preprocessed'].apply(lambda x : p3.reduce(x))\n",
    "brown_corpus_docs['preprocessed'] = brown_corpus_docs['preprocessed'].apply(lambda x : p4.fromList(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read queries\n",
    "queries_json = json.load(open( \".\\cranfield\\cran_queries.json\", 'r'))[:]\n",
    "query_ids, queries = [item[\"query number\"] for item in queries_json], \\\n",
    "                        [item[\"query\"] for item in queries_json]\n",
    "\n",
    "# Read documents\n",
    "docs_json = json.load(open(\".\\cranfield\\cran_docs.json\", 'r'))[:]\n",
    "doc_ids, docs = [item[\"id\"] for item in docs_json], \\\n",
    "                        [item[\"body\"] for item in docs_json]\n",
    "# Loading preprocessed queries\n",
    "Preprocessed_queries = json.load(open('stopword_removed_queries.txt', 'r'))\n",
    "# Loading Preprocessed docs\n",
    "Preprocessed_docs = json.load(open('stopword_removed_docs.txt', 'r'))\n",
    "\n",
    "qrels = json.load(open( \"./cranfield/cran_qrels.json\", 'r'))[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_corpus = brown_corpus_docs['preprocessed'].tolist()\n",
    "total_corpus = Preprocessed_docs + Preprocessed_queries + new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference\n",
    "evaluator = Evaluation()\n",
    "def Evaluation_metrics(doc_IDs_ordered, query_ids, qrels, n_comp, op_folder = './',save_results = 2, verbose = 1):\n",
    "    \"\"\"\n",
    "    save_results : 0    ===> don't save anything\n",
    "                 : 1    ===> just save results\n",
    "                 : > 2  ===> save plots also\n",
    "    \"\"\"\n",
    "    precisions, recalls, fscores, MAPs, nDCGs = [], [], [], [], []\n",
    "    for k in range(1,11):\n",
    "        precision = evaluator.meanPrecision(\n",
    "            doc_IDs_ordered, query_ids, qrels, k)\n",
    "        precisions.append(precision)\n",
    "        recall = evaluator.meanRecall(\n",
    "            doc_IDs_ordered, query_ids, qrels, k)\n",
    "        recalls.append(recall)\n",
    "        fscore = evaluator.meanFscore(\n",
    "            doc_IDs_ordered, query_ids, qrels, k)\n",
    "        fscores.append(fscore)\n",
    "\n",
    "        MAP = evaluator.meanAveragePrecision(\n",
    "            doc_IDs_ordered, query_ids, qrels, k)\n",
    "        MAPs.append(MAP)\n",
    "        nDCG = evaluator.meanNDCG(\n",
    "            doc_IDs_ordered, query_ids, qrels, k)\n",
    "        nDCGs.append(nDCG)\n",
    "        if (verbose):\n",
    "            print(\"Precision, Recall and F-score @ \" +  \n",
    "                str(k) + \" : \" + str(precision) + \", \" + str(recall) + \n",
    "                \", \" + str(fscore))\n",
    "            print(\"MAP, nDCG @ \" +  \n",
    "                str(k) + \" : \" + str(MAP) + \", \" + str(nDCG))\n",
    "        if (save_results > 0):\n",
    "        # saving the results\n",
    "            with open(op_folder+'Results/LSA_'+str(n_comp)+'.txt', 'a') as f:\n",
    "                f.write(str(k) + \" , \" + str(precision) + \", \" + str(recall) + \n",
    "                        \", \" + str(fscore)+\", \"+str(MAP) + \", \" + str(nDCG)+'\\n')\n",
    "            with open(op_folder+'Results/metrics_'+str(k)+'.txt', 'a') as f:\n",
    "                f.write(str(n_comp) + \" , \" + str(precision) + \", \" + str(recall) + \n",
    "                        \", \" + str(fscore)+\", \"+str(MAP) + \", \" + str(nDCG)+'\\n')\n",
    "            \n",
    "    # Plot the metrics and save plot \n",
    "    if (save_results > 1):\n",
    "        plt.figure()\n",
    "        plt.plot(range(1, 11), precisions, label=\"Precision\")\n",
    "        plt.plot(range(1, 11), recalls, label=\"Recall\")\n",
    "        plt.plot(range(1, 11), fscores, label=\"F-Score\")\n",
    "        plt.plot(range(1, 11), MAPs, label=\"MAP\")\n",
    "        plt.plot(range(1, 11), nDCGs, label=\"nDCG\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Evaluation Metrics - LSA \"+str(n_comp))\n",
    "        plt.xlabel(\"k\")\n",
    "        plt.savefig(op_folder + \"Plots/LSA_\"+str(n_comp)+\".png\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-idf using Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "merged_total_docs = []\n",
    "for docs in total_corpus:\n",
    "    merged = ' '.join(list(itertools.chain(*docs)))\n",
    "    merged_total_docs += [merged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of docs and queries included are 2125 and Vocabulary size is 34872\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_1</th>\n",
       "      <th>doc_2</th>\n",
       "      <th>doc_3</th>\n",
       "      <th>doc_4</th>\n",
       "      <th>doc_5</th>\n",
       "      <th>doc_6</th>\n",
       "      <th>doc_7</th>\n",
       "      <th>doc_8</th>\n",
       "      <th>doc_9</th>\n",
       "      <th>doc_10</th>\n",
       "      <th>...</th>\n",
       "      <th>doc_2116</th>\n",
       "      <th>doc_2117</th>\n",
       "      <th>doc_2118</th>\n",
       "      <th>doc_2119</th>\n",
       "      <th>doc_2120</th>\n",
       "      <th>doc_2121</th>\n",
       "      <th>doc_2122</th>\n",
       "      <th>doc_2123</th>\n",
       "      <th>doc_2124</th>\n",
       "      <th>doc_2125</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.107015</td>\n",
       "      <td>0.015161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0005</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000degree</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2125 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           doc_1  doc_2  doc_3  doc_4  doc_5  doc_6  doc_7  doc_8  doc_9  \\\n",
       "00           0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "000          0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "0001         0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "0005         0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "000degree    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "           doc_10  ...  doc_2116  doc_2117  doc_2118  doc_2119  doc_2120  \\\n",
       "00            0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "000           0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "0001          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "0005          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "000degree     0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "           doc_2121  doc_2122  doc_2123  doc_2124  doc_2125  \n",
       "00         0.000000  0.000000       0.0       0.0       0.0  \n",
       "000        0.107015  0.015161       0.0       0.0       0.0  \n",
       "0001       0.000000  0.000000       0.0       0.0       0.0  \n",
       "0005       0.000000  0.000000       0.0       0.0       0.0  \n",
       "000degree  0.000000  0.000000       0.0       0.0       0.0  \n",
       "\n",
       "[5 rows x 2125 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(merged_total_docs)\n",
    "print(\"Total number of docs and queries included are {} and Vocabulary size is {}\".format(X.shape[0],X.shape[1]))\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "columns = ['doc_' + str(i) for i in range(1,X.shape[0]+1)]\n",
    "\n",
    "tfidf_df = pd.DataFrame(X.T.todense(), index=feature_names, columns=columns)\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_docs = tfidf_df[columns[:len(Preprocessed_docs)]].values\n",
    "vec_rep_queries = tfidf_df[columns[len(Preprocessed_docs):len(Preprocessed_docs) + len(Preprocessed_queries)]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_map = build_word_index(Preprocessed_docs, doc_ids)\n",
    "\n",
    "# # TF-IDF representation\n",
    "# tf_idf_docs = TF_IDF(Preprocessed_docs, doc_ids, word_map, normalize = True)\n",
    "# vec_rep_queries = TF_IDF(Preprocessed_queries, query_ids, word_map, normalize = True, is_queries= True)\n",
    "\n",
    "cosine_sim = np.matmul(tf_idf_docs.T, vec_rep_queries )\n",
    "doc_IDs_ordered = (np.argsort(cosine_sim,axis=0)+1)[::-1].T.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision, Recall and F-score @ 1 : 0.6844444444444444, 0.11682720282918488, 0.19122503049513545\n",
      "MAP, nDCG @ 1 : 0.6844444444444444, 0.5185185185185185\n",
      "Precision, Recall and F-score @ 2 : 0.56, 0.18175337913453635, 0.25746452014559285\n",
      "MAP, nDCG @ 2 : 0.7177777777777777, 0.41077004154365504\n",
      "Precision, Recall and F-score @ 3 : 0.5096296296296297, 0.23954680755481814, 0.30282208900281954\n",
      "MAP, nDCG @ 3 : 0.7288888888888893, 0.3931538951309575\n",
      "Precision, Recall and F-score @ 4 : 0.45555555555555555, 0.28427692701300766, 0.32434011316712025\n",
      "MAP, nDCG @ 4 : 0.7204938271604941, 0.3867247107075635\n",
      "Precision, Recall and F-score @ 5 : 0.4124444444444446, 0.3134559807241778, 0.32864933269170726\n",
      "MAP, nDCG @ 5 : 0.7176234567901238, 0.38842838741781793\n",
      "Precision, Recall and F-score @ 6 : 0.38148148148148125, 0.3410360364708999, 0.33240844115561025\n",
      "MAP, nDCG @ 6 : 0.7129086419753086, 0.39511801882172287\n",
      "Precision, Recall and F-score @ 7 : 0.3612698412698417, 0.3714738821172082, 0.3380591963497686\n",
      "MAP, nDCG @ 7 : 0.6900790123456788, 0.4027527973253531\n",
      "Precision, Recall and F-score @ 8 : 0.3427777777777778, 0.39862309540821694, 0.34056106853831697\n",
      "MAP, nDCG @ 8 : 0.6741448727639201, 0.41042068111415025\n",
      "Precision, Recall and F-score @ 9 : 0.3195061728395065, 0.4114018753191131, 0.33242990645229564\n",
      "MAP, nDCG @ 9 : 0.6651679390274626, 0.4128556406905995\n",
      "Precision, Recall and F-score @ 10 : 0.2986666666666667, 0.42442227856758646, 0.32435244987939316\n",
      "MAP, nDCG @ 10 : 0.6604521961871165, 0.4158336252111869\n"
     ]
    }
   ],
   "source": [
    "Evaluation_metrics(doc_IDs_ordered, query_ids, qrels,n_comp = 0,save_results=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA Tuning with various components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LSA with 20 in progress\n",
      "\n",
      "Precision, Recall and F-score @ 1 : 0.22666666666666666, 0.02905521288328305, 0.049906078568340395\n",
      "MAP, nDCG @ 1 : 0.22666666666666666, 0.11555555555555551\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './tf_idf_matrix/Results/LSA_20.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-18936382e4f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mdoc_IDs_ordered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcosine_sim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nLSA with \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_comp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" in progress\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mEvaluation_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_IDs_ordered\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqrels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_comp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_folder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./tf_idf_matrix/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-b9ec96f1b1e2>\u001b[0m in \u001b[0;36mEvaluation_metrics\u001b[1;34m(doc_IDs_ordered, query_ids, qrels, n_comp, op_folder, save_results, verbose)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msave_results\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m# saving the results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_folder\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'Results/LSA_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_comp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m                 f.write(str(k) + \" , \" + str(precision) + \", \" + str(recall) + \n\u001b[0;32m     37\u001b[0m                         \", \" + str(fscore)+\", \"+str(MAP) + \", \" + str(nDCG)+'\\n')\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './tf_idf_matrix/Results/LSA_20.txt'"
     ]
    }
   ],
   "source": [
    "components_used = [20, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "# components_used = [1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000]\n",
    "# components_used = [1,10]\n",
    "for n_comp in components_used:\n",
    "    svd = TruncatedSVD(n_components=n_comp)\n",
    "    svd.fit(tf_idf_docs.T)\n",
    "    tr_docs = svd.transform(tf_idf_docs.T).T\n",
    "    # tr_docs.shape\n",
    "    qr_tr = svd.transform(vec_rep_queries.T).T\n",
    "    # qr_tr.shape\n",
    "    cosine_sim = np.matmul(tr_docs.T, qr_tr )\n",
    "    doc_IDs_ordered = (np.argsort(cosine_sim,axis=0)+1)[::-1].T.tolist()\n",
    "    print(\"\\nLSA with \"+str(n_comp)+\" in progress\\n\")\n",
    "    Evaluation_metrics(doc_IDs_ordered, query_ids, qrels, n_comp, op_folder='./tf_idf_matrix/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering documents to reduce search time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters= 3, random_state=0)\n",
    "km.fit(tf_idf_docs.T)\n",
    "km.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_doc_ids = {}\n",
    "for i in range(1400):\n",
    "    try :\n",
    "        cluster_doc_ids[km.labels_[i]] += [i]\n",
    "    except :\n",
    "        cluster_doc_ids[km.labels_[i]] = [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_query = np.matmul(vec_rep_queries.T, km.cluster_centers_.T)\n",
    "# cluster_query = np.argmax(cluster_query, axis = 1)\n",
    "# cluster_query.shape\n",
    "\n",
    "# normal method of retrieval\n",
    "tic = time.time()\n",
    "cosine_sim = []\n",
    "for i in range(1400):\n",
    "    cosine_sim.append(np.matmul(tf_idf_docs[:,i].T, vec_rep_queries[:,0]))\n",
    "cosine_sim = np.array(cosine_sim)\n",
    "doc_IDs_ordered = (np.argsort(cosine_sim,axis=0)+1)[::-1].T.tolist()\n",
    "print(len(doc_IDs_ordered))\n",
    "toc = time.time()\n",
    "print(\"without clustering, Retrieval time : \"+str(toc-tic))\n",
    "\n",
    "# clustering method\n",
    "tic = time.time()\n",
    "cluster = np.argmax(np.matmul(vec_rep_queries[:,0], km.cluster_centers_.T))\n",
    "cluster_docs = tf_idf_docs[:, cluster_doc_ids[cluster]]\n",
    "# cosine_sim = np.matmul(tf_idf_docs[:, cluster_doc_ids[cluster]].T,vec_rep_queries[:,0])\n",
    "cosine_sim = np.matmul(cluster_docs.T,vec_rep_queries[:,0])\n",
    "doc_IDs_ordered_clus = (np.argsort(cosine_sim,axis=0))[::-1].T.tolist()\n",
    "print(len(doc_IDs_ordered_clus))\n",
    "doc_IDs_ordered = np.array(cluster_doc_ids[cluster])[doc_IDs_ordered_clus]+1\n",
    "toc = time.time()\n",
    "print(\"clustering method, Retrieval time : \"+str(toc-tic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking for the compensation made\n",
    "doc_IDs_ordered = []\n",
    "for qry_idx in range(225):\n",
    "    cluster = np.argmax(np.matmul(vec_rep_queries[:,qry_idx], km.cluster_centers_.T))\n",
    "    cluster_docs = tf_idf_docs[:, cluster_doc_ids[cluster]]\n",
    "    # cosine_sim = np.matmul(tf_idf_docs[:, cluster_doc_ids[cluster]].T,vec_rep_queries[:,0])\n",
    "    cosine_sim = np.matmul(cluster_docs.T,vec_rep_queries[:,qry_idx])\n",
    "    doc_IDs_ordered_clus = (np.argsort(cosine_sim,axis=0))[::-1].T.tolist()\n",
    "    # print(len(doc_IDs_ordered_clus))\n",
    "    doc_IDs_ordered.append((np.array(cluster_doc_ids[cluster])[doc_IDs_ordered_clus]+1).tolist())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluation_metrics(doc_IDs_ordered, query_ids, qrels,n_comp = 0,save_results=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "- Clustering reduced the retrieval time by 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Expansion -- distributional word similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_sentences = []\n",
    "for doc in Preprocessed_docs:\n",
    "    for sent in doc:\n",
    "        All_sentences.append(sent)\n",
    "All_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(min_count=1,\n",
    "                window = 3,\n",
    "                size = 500,\n",
    "                sample = 6e-5)\n",
    "t = time.time()\n",
    "model.build_vocab(All_sentences, progress_per = 1000)\n",
    "print('Time to build vocab: {} mins'.format(round((time.time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "\n",
    "model.train(All_sentences, total_examples=model.corpus_count, epochs=50, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time.time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.array(model.wv.most_similar(positive=[\"good\"]))[:,0].tolist()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_expansion(query, wv_model,v=1):\n",
    "    \"\"\"\n",
    "    query : query to be expanded (a list of lists, where each sublist is a sentence)\n",
    "    wv_model : word2vec trained model\n",
    "    v : top v similar words taken into consideration\n",
    "    \"\"\"\n",
    "    expanded_query = query.copy()\n",
    "    for sent in query:\n",
    "        \n",
    "        for word in sent:\n",
    "            \n",
    "            try:\n",
    "                expanded_query.append(np.array(wv_model.wv.most_similar(positive=[word],topn = v))[:,0].tolist())\n",
    "            except:\n",
    "                pass\n",
    "    return expanded_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample return of query_expansion result\n",
    "query_expansion([[\"investigation\",\"bad\"]],model,v=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Preprocessed_queries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_queries = []\n",
    "for query in Preprocessed_queries:\n",
    "    expanded_queries.append(query_expansion(query,model2, v=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_rep_exp_queries = TF_IDF(expanded_queries, query_ids, word_map, is_queries=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = np.matmul(tf_idf_docs.T, vec_rep_exp_queries )\n",
    "doc_IDs_ordered = (np.argsort(cosine_sim,axis=0)+1)[::-1].T.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluation_metrics(doc_IDs_ordered, query_ids, qrels,n_comp = 0,save_results=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "model2 = Word2Vec(brown.sents(),min_count=1,\n",
    "                window = 2,\n",
    "                size = 200,\n",
    "                sample = 6e-5)\n",
    "# model.build_vocab(, progress_per = 10000)\n",
    "print('Time to build vocab: {} mins'.format(round((time.time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "\n",
    "model2.train(All_sentences, total_examples=model.corpus_count, epochs=50, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time.time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.array(model2.wv.most_similar(positive=[\"good\"]))[:,0].tolist()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentences = All_sentences+brown.sents()\n",
    "Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "model2 = Word2Vec(Sentences,min_count=1,\n",
    "                window = 2,\n",
    "                size = 300,\n",
    "                sample = 6e-5)\n",
    "# model.build_vocab(, progress_per = 10000)\n",
    "print('Time to build vocab: {} mins'.format(round((time.time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "\n",
    "model2.train(All_sentences, total_examples=model.corpus_count, epochs=50, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time.time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
